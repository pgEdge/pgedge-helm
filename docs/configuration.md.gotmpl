This chart is built around managing each pgEdge node as a [CloudNativePG](https://cloudnative-pg.io/) `Cluster`.

The chart contains a default `clusterSpec` in `values.yaml` which defines the required configuration for deploying pgEdge with CloudNativePG, including:

- deploying with the [pgEdge Enterprise Postgres Images](https://github.com/pgedge/postgres-images).
- loading and initializing required extensions for pgEdge Distributed Postgres.
- setting up required PostgreSQL configuration parameters.
- configuring client certificate authentication for managed users (app, admin, streaming_replica).
- allowing local connections for the app and admin users for testing / development purposes.

The simplest example values file, which deploys a single primary instance for each node, looks like this:

```yaml
pgEdge:
  appName: pgedge
  nodes:
    - name: n1
      hostname: pgedge-n1-rw
    - name: n2
      hostname: pgedge-n2-rw
    - name: n3
      hostname: pgedge-n3-rw
  clusterSpec:
    storage:
      size: 1Gi
```

As shown, The default `clusterSpec` can be overridden for all nodes with specific configuration required for your Kubernetes setup.

You can also override the `clusterSpec` for specific nodes if you require more granular control.

For example, to create a 3-node cluster with 3 instances on node `n1` and single instances on nodes `n2` and `n3`, you could use:

```yaml
pgEdge:
  appName: pgedge
  nodes:
    - name: n1
      hostname: pgedge-n1-rw
      clusterSpec:
        instances: 3
        postgresql:
          synchronous:
            method: any
            number: 1
            dataDurability: required
    - name: n2
      hostname: pgedge-n2-rw
    - name: n3
      hostname: pgedge-n3-rw
```

This override behavior is enabled via `mergeOverwrite` in Helm. You should be mindful that lists are replaced, not merged.

If you override a list in the `clusterSpec` for a node, you must include all required elements in that list, pulling from the values file example, and using `helm template` to verify your configuration.

For more information on configuring CloudNativePG, be sure to reference their documentation at https://cloudnative-pg.io/docs/

## Spock configuration

This chart contains a python job to initialize Spock multi-master replication across all nodes once they are all available.

This job runs by default, waiting for any clusters associated with the current deployment to be ready before performing initialization.

If you wish to disable this behavior, you can set `pgEdge.initSpock` to `false`.

### snowflake.node and lolor.node

This chart automatically configures `snowflake.node` and `lolor.node` based on the `name` property of each node.

For example, a node named `n1` will have the following Postgres configuration applied to the node to ensure snowflake and lolor are configured appropriately:

```yaml
  postgresql:
    parameters:
      ...
      lolor.node: "1"
      snowflake.node: "1"
```

If you wish to override this behavior, or plan to utilize alternate naming schemes for your node, you can set the `ordinal` property for each node:

```yaml
pgEdge:
  appName: pgedge
  nodes:
    - name: a
      hostname: pgedge-a-rw
      ordinal: 1
    - name: b
      hostname: pgedge-b-rw
      ordinal: 2
    - name: c
      hostname: pgedge-c-rw
      ordinal: 3
  clusterSpec:
    storage:
      size: 1Gi
```

## Values reference

You can customize this Helm chart by specifying configuration parameters in your `values.yaml` file. 

The following table lists all available options and their descriptions.

{{ template "chart.valuesTable" . }}